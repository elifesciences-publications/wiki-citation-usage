{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Section Heirarchy from WPM pages\n",
    "Data extracted from public HTML pages and written to hive. Extraction details:\n",
    " - began: 2019-04-23T23:38 UTC\n",
    " - ended: 2019-04-24T01:31 UTC\n",
    " - errors: 15 WPM pages could not be fetched (out of 32718 total)\n",
    " \n",
    "Pages with extraction errors (missing pages)\n",
    "\n",
    "| `page_id`  |\n",
    "|----------|\n",
    "| 18306469 |\n",
    "| 23607873 |\n",
    "| 31438049 |\n",
    "| 36464554 |\n",
    "| 4222711  |\n",
    "| 45679508 |\n",
    "| 4625273  |\n",
    "| 49095500 |\n",
    "| 53275764 |\n",
    "| 59824954 |\n",
    "| 60122321 |\n",
    "| 60228761 |\n",
    "| 60398972 |\n",
    "| 60410010 |\n",
    "| 9429312  | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 records fetched\n",
      "2000 records fetched\n",
      "3000 records fetched\n",
      "4000 records fetched\n",
      "5000 records fetched\n",
      "6000 records fetched\n",
      "7000 records fetched\n",
      "8000 records fetched\n",
      "9000 records fetched\n",
      "10000 records fetched\n",
      "11000 records fetched\n",
      "12000 records fetched\n",
      "13000 records fetched\n",
      "14000 records fetched\n",
      "15000 records fetched\n",
      "16000 records fetched\n",
      "17000 records fetched\n",
      "18000 records fetched\n",
      "19000 records fetched\n",
      "20000 records fetched\n",
      "21000 records fetched\n",
      "22000 records fetched\n",
      "23000 records fetched\n",
      "24000 records fetched\n",
      "25000 records fetched\n",
      "26000 records fetched\n",
      "27000 records fetched\n",
      "28000 records fetched\n",
      "29000 records fetched\n",
      "30000 records fetched\n",
      "31000 records fetched\n",
      "32000 records fetched\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "base_path = '/home/ryanmax/section-extraction/'\n",
    "base_url = 'https://en.wikipedia.org/?curid='\n",
    "\n",
    "output_file = open(base_path + 'output.txt', 'w')\n",
    "\n",
    "input_file = 'wpm_page_ids.txt'\n",
    "#input_file = 'ids.txt'\n",
    "\n",
    "ids = [id.rstrip('\\n') for id in open(base_path + input_file)]\n",
    "\n",
    "def extract_h2(id):\n",
    "    dt = datetime.datetime.now().isoformat()\n",
    "    try:\n",
    "        wpm_page = urllib.request.urlopen(base_url + id)\n",
    "    except urllib.error.HTTPError:\n",
    "        output_file.write(id + \"\\tERROR\\tERROR\\t\" + dt + \"\\n\")\n",
    "        return\n",
    "    soup = BeautifulSoup(wpm_page.read(), features=\"lxml\")\n",
    "\n",
    "    h2 = \"\"\n",
    "    for span in soup.find_all(class_=\"mw-headline\"):\n",
    "        if('h2' == span.parent.name):\n",
    "            h2 = span.get('id')\n",
    "        output_file.write(id + \"\\t\" + h2 + \"\\t\" + span.get('id') + \"\\t\" + dt + \"\\n\")\n",
    "\n",
    "output_file.write(\"page_id\\tsection_h2\\tsection_id\\tdt\\n\")\n",
    "\n",
    "i = 0\n",
    "for id in ids:\n",
    "    i += 1\n",
    "    extract_h2(id)\n",
    "    if (i%1000 == 0):\n",
    "        print(str(i) + \" records fetched\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
